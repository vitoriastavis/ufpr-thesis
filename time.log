nohup: ignoring input
w2v-bpe
Done w2v-bpe 1
w2v-bpe
Done w2v-bpe 2
w2v-bpe
Done w2v-bpe 3
w2v-bpe
Done w2v-bpe 4
w2v-bpe
Done w2v-bpe 5
w2v-bpe
Done w2v-bpe 6
w2v-bpe
Done w2v-bpe 7
w2v-bpe
Done w2v-bpe 8
w2v-bpe
Done w2v-bpe 9
w2v-bpe
Done w2v-bpe 10
w2v-bpe
Done w2v-bpe 11
w2v-bpe
Done w2v-bpe 12
w2v-bpe
Done w2v-bpe 13
w2v-bpe
Done w2v-bpe 14
w2v-bpe
Done w2v-bpe 15
w2v-bpe
Done w2v-bpe 16
w2v-kmer
Done w2v-kmer 1
w2v-kmer
Done w2v-kmer 2
w2v-kmer
Done w2v-kmer 3
w2v-kmer
Done w2v-kmer 4
w2v-kmer
Done w2v-kmer 5
w2v-kmer
Done w2v-kmer 6
w2v-kmer
Done w2v-kmer 7
w2v-kmer
Done w2v-kmer 8
w2v-kmer
Done w2v-kmer 9
w2v-kmer
Done w2v-kmer 10
w2v-kmer
Done w2v-kmer 11
w2v-kmer
Done w2v-kmer 12
w2v-kmer
Done w2v-kmer 13
w2v-kmer
Done w2v-kmer 14
w2v-kmer
Done w2v-kmer 15
w2v-kmer
Done w2v-kmer 16
dnabert1-pretrained
Done dnabert1-pretrained 1
dnabert1-pretrained
Done dnabert1-pretrained 2
dnabert1-pretrained
Done dnabert1-pretrained 3
dnabert1-pretrained
Done dnabert1-pretrained 4
dnabert1-finetuned-motifs
Done dnabert1-finetuned-motifs 1
dnabert1-finetuned-motifs
Done dnabert1-finetuned-motifs 2
dnabert1-finetuned-motifs
Done dnabert1-finetuned-motifs 3
dnabert1-finetuned-motifs
Done dnabert1-finetuned-motifs 4
dnabert2-pretrained
/home/stavisa/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).
  warnings.warn(
Some weights of BertModel were not initialized from the model checkpoint at zhihan1996/DNABERT-2-117M and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Done dnabert2-pretrained 1
dnabert2-pretrained
Some weights of BertModel were not initialized from the model checkpoint at zhihan1996/DNABERT-2-117M and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Done dnabert2-pretrained 2
dnabert2-pretrained
Some weights of BertModel were not initialized from the model checkpoint at zhihan1996/DNABERT-2-117M and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Done dnabert2-pretrained 3
dnabert2-pretrained
Some weights of BertModel were not initialized from the model checkpoint at zhihan1996/DNABERT-2-117M and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Done dnabert2-pretrained 4
dnabert2-finetuned-cancer
Done dnabert2-finetuned-cancer 1
dnabert2-finetuned-cancer
Done dnabert2-finetuned-cancer 2
dnabert2-finetuned-cancer
Done dnabert2-finetuned-cancer 3
dnabert2-finetuned-cancer
Done dnabert2-finetuned-cancer 4
grover-pretrained
Some weights of BertModel were not initialized from the model checkpoint at PoetschLab/GROVER and are newly initialized: ['bert.encoder.layer.0.attention.self.Wqkv.bias', 'bert.encoder.layer.0.attention.self.Wqkv.weight', 'bert.encoder.layer.0.mlp.gated_layers.weight', 'bert.encoder.layer.0.mlp.layernorm.bias', 'bert.encoder.layer.0.mlp.layernorm.weight', 'bert.encoder.layer.0.mlp.wo.bias', 'bert.encoder.layer.0.mlp.wo.weight', 'bert.encoder.layer.1.attention.self.Wqkv.bias', 'bert.encoder.layer.1.attention.self.Wqkv.weight', 'bert.encoder.layer.1.mlp.gated_layers.weight', 'bert.encoder.layer.1.mlp.layernorm.bias', 'bert.encoder.layer.1.mlp.layernorm.weight', 'bert.encoder.layer.1.mlp.wo.bias', 'bert.encoder.layer.1.mlp.wo.weight', 'bert.encoder.layer.10.attention.self.Wqkv.bias', 'bert.encoder.layer.10.attention.self.Wqkv.weight', 'bert.encoder.layer.10.mlp.gated_layers.weight', 'bert.encoder.layer.10.mlp.layernorm.bias', 'bert.encoder.layer.10.mlp.layernorm.weight', 'bert.encoder.layer.10.mlp.wo.bias', 'bert.encoder.layer.10.mlp.wo.weight', 'bert.encoder.layer.11.attention.self.Wqkv.bias', 'bert.encoder.layer.11.attention.self.Wqkv.weight', 'bert.encoder.layer.11.mlp.gated_layers.weight', 'bert.encoder.layer.11.mlp.layernorm.bias', 'bert.encoder.layer.11.mlp.layernorm.weight', 'bert.encoder.layer.11.mlp.wo.bias', 'bert.encoder.layer.11.mlp.wo.weight', 'bert.encoder.layer.2.attention.self.Wqkv.bias', 'bert.encoder.layer.2.attention.self.Wqkv.weight', 'bert.encoder.layer.2.mlp.gated_layers.weight', 'bert.encoder.layer.2.mlp.layernorm.bias', 'bert.encoder.layer.2.mlp.layernorm.weight', 'bert.encoder.layer.2.mlp.wo.bias', 'bert.encoder.layer.2.mlp.wo.weight', 'bert.encoder.layer.3.attention.self.Wqkv.bias', 'bert.encoder.layer.3.attention.self.Wqkv.weight', 'bert.encoder.layer.3.mlp.gated_layers.weight', 'bert.encoder.layer.3.mlp.layernorm.bias', 'bert.encoder.layer.3.mlp.layernorm.weight', 'bert.encoder.layer.3.mlp.wo.bias', 'bert.encoder.layer.3.mlp.wo.weight', 'bert.encoder.layer.4.attention.self.Wqkv.bias', 'bert.encoder.layer.4.attention.self.Wqkv.weight', 'bert.encoder.layer.4.mlp.gated_layers.weight', 'bert.encoder.layer.4.mlp.layernorm.bias', 'bert.encoder.layer.4.mlp.layernorm.weight', 'bert.encoder.layer.4.mlp.wo.bias', 'bert.encoder.layer.4.mlp.wo.weight', 'bert.encoder.layer.5.attention.self.Wqkv.bias', 'bert.encoder.layer.5.attention.self.Wqkv.weight', 'bert.encoder.layer.5.mlp.gated_layers.weight', 'bert.encoder.layer.5.mlp.layernorm.bias', 'bert.encoder.layer.5.mlp.layernorm.weight', 'bert.encoder.layer.5.mlp.wo.bias', 'bert.encoder.layer.5.mlp.wo.weight', 'bert.encoder.layer.6.attention.self.Wqkv.bias', 'bert.encoder.layer.6.attention.self.Wqkv.weight', 'bert.encoder.layer.6.mlp.gated_layers.weight', 'bert.encoder.layer.6.mlp.layernorm.bias', 'bert.encoder.layer.6.mlp.layernorm.weight', 'bert.encoder.layer.6.mlp.wo.bias', 'bert.encoder.layer.6.mlp.wo.weight', 'bert.encoder.layer.7.attention.self.Wqkv.bias', 'bert.encoder.layer.7.attention.self.Wqkv.weight', 'bert.encoder.layer.7.mlp.gated_layers.weight', 'bert.encoder.layer.7.mlp.layernorm.bias', 'bert.encoder.layer.7.mlp.layernorm.weight', 'bert.encoder.layer.7.mlp.wo.bias', 'bert.encoder.layer.7.mlp.wo.weight', 'bert.encoder.layer.8.attention.self.Wqkv.bias', 'bert.encoder.layer.8.attention.self.Wqkv.weight', 'bert.encoder.layer.8.mlp.gated_layers.weight', 'bert.encoder.layer.8.mlp.layernorm.bias', 'bert.encoder.layer.8.mlp.layernorm.weight', 'bert.encoder.layer.8.mlp.wo.bias', 'bert.encoder.layer.8.mlp.wo.weight', 'bert.encoder.layer.9.attention.self.Wqkv.bias', 'bert.encoder.layer.9.attention.self.Wqkv.weight', 'bert.encoder.layer.9.mlp.gated_layers.weight', 'bert.encoder.layer.9.mlp.layernorm.bias', 'bert.encoder.layer.9.mlp.layernorm.weight', 'bert.encoder.layer.9.mlp.wo.bias', 'bert.encoder.layer.9.mlp.wo.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/stavisa/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:433: UserWarning: Increasing alibi size from 0 to 29
  warnings.warn(
/home/stavisa/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:433: UserWarning: Increasing alibi size from 29 to 30
  warnings.warn(
/home/stavisa/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:433: UserWarning: Increasing alibi size from 30 to 32
  warnings.warn(
Done grover-pretrained 1
grover-pretrained
Some weights of BertModel were not initialized from the model checkpoint at PoetschLab/GROVER and are newly initialized: ['bert.encoder.layer.0.attention.self.Wqkv.bias', 'bert.encoder.layer.0.attention.self.Wqkv.weight', 'bert.encoder.layer.0.mlp.gated_layers.weight', 'bert.encoder.layer.0.mlp.layernorm.bias', 'bert.encoder.layer.0.mlp.layernorm.weight', 'bert.encoder.layer.0.mlp.wo.bias', 'bert.encoder.layer.0.mlp.wo.weight', 'bert.encoder.layer.1.attention.self.Wqkv.bias', 'bert.encoder.layer.1.attention.self.Wqkv.weight', 'bert.encoder.layer.1.mlp.gated_layers.weight', 'bert.encoder.layer.1.mlp.layernorm.bias', 'bert.encoder.layer.1.mlp.layernorm.weight', 'bert.encoder.layer.1.mlp.wo.bias', 'bert.encoder.layer.1.mlp.wo.weight', 'bert.encoder.layer.10.attention.self.Wqkv.bias', 'bert.encoder.layer.10.attention.self.Wqkv.weight', 'bert.encoder.layer.10.mlp.gated_layers.weight', 'bert.encoder.layer.10.mlp.layernorm.bias', 'bert.encoder.layer.10.mlp.layernorm.weight', 'bert.encoder.layer.10.mlp.wo.bias', 'bert.encoder.layer.10.mlp.wo.weight', 'bert.encoder.layer.11.attention.self.Wqkv.bias', 'bert.encoder.layer.11.attention.self.Wqkv.weight', 'bert.encoder.layer.11.mlp.gated_layers.weight', 'bert.encoder.layer.11.mlp.layernorm.bias', 'bert.encoder.layer.11.mlp.layernorm.weight', 'bert.encoder.layer.11.mlp.wo.bias', 'bert.encoder.layer.11.mlp.wo.weight', 'bert.encoder.layer.2.attention.self.Wqkv.bias', 'bert.encoder.layer.2.attention.self.Wqkv.weight', 'bert.encoder.layer.2.mlp.gated_layers.weight', 'bert.encoder.layer.2.mlp.layernorm.bias', 'bert.encoder.layer.2.mlp.layernorm.weight', 'bert.encoder.layer.2.mlp.wo.bias', 'bert.encoder.layer.2.mlp.wo.weight', 'bert.encoder.layer.3.attention.self.Wqkv.bias', 'bert.encoder.layer.3.attention.self.Wqkv.weight', 'bert.encoder.layer.3.mlp.gated_layers.weight', 'bert.encoder.layer.3.mlp.layernorm.bias', 'bert.encoder.layer.3.mlp.layernorm.weight', 'bert.encoder.layer.3.mlp.wo.bias', 'bert.encoder.layer.3.mlp.wo.weight', 'bert.encoder.layer.4.attention.self.Wqkv.bias', 'bert.encoder.layer.4.attention.self.Wqkv.weight', 'bert.encoder.layer.4.mlp.gated_layers.weight', 'bert.encoder.layer.4.mlp.layernorm.bias', 'bert.encoder.layer.4.mlp.layernorm.weight', 'bert.encoder.layer.4.mlp.wo.bias', 'bert.encoder.layer.4.mlp.wo.weight', 'bert.encoder.layer.5.attention.self.Wqkv.bias', 'bert.encoder.layer.5.attention.self.Wqkv.weight', 'bert.encoder.layer.5.mlp.gated_layers.weight', 'bert.encoder.layer.5.mlp.layernorm.bias', 'bert.encoder.layer.5.mlp.layernorm.weight', 'bert.encoder.layer.5.mlp.wo.bias', 'bert.encoder.layer.5.mlp.wo.weight', 'bert.encoder.layer.6.attention.self.Wqkv.bias', 'bert.encoder.layer.6.attention.self.Wqkv.weight', 'bert.encoder.layer.6.mlp.gated_layers.weight', 'bert.encoder.layer.6.mlp.layernorm.bias', 'bert.encoder.layer.6.mlp.layernorm.weight', 'bert.encoder.layer.6.mlp.wo.bias', 'bert.encoder.layer.6.mlp.wo.weight', 'bert.encoder.layer.7.attention.self.Wqkv.bias', 'bert.encoder.layer.7.attention.self.Wqkv.weight', 'bert.encoder.layer.7.mlp.gated_layers.weight', 'bert.encoder.layer.7.mlp.layernorm.bias', 'bert.encoder.layer.7.mlp.layernorm.weight', 'bert.encoder.layer.7.mlp.wo.bias', 'bert.encoder.layer.7.mlp.wo.weight', 'bert.encoder.layer.8.attention.self.Wqkv.bias', 'bert.encoder.layer.8.attention.self.Wqkv.weight', 'bert.encoder.layer.8.mlp.gated_layers.weight', 'bert.encoder.layer.8.mlp.layernorm.bias', 'bert.encoder.layer.8.mlp.layernorm.weight', 'bert.encoder.layer.8.mlp.wo.bias', 'bert.encoder.layer.8.mlp.wo.weight', 'bert.encoder.layer.9.attention.self.Wqkv.bias', 'bert.encoder.layer.9.attention.self.Wqkv.weight', 'bert.encoder.layer.9.mlp.gated_layers.weight', 'bert.encoder.layer.9.mlp.layernorm.bias', 'bert.encoder.layer.9.mlp.layernorm.weight', 'bert.encoder.layer.9.mlp.wo.bias', 'bert.encoder.layer.9.mlp.wo.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Done grover-pretrained 2
grover-pretrained
Some weights of BertModel were not initialized from the model checkpoint at PoetschLab/GROVER and are newly initialized: ['bert.encoder.layer.0.attention.self.Wqkv.bias', 'bert.encoder.layer.0.attention.self.Wqkv.weight', 'bert.encoder.layer.0.mlp.gated_layers.weight', 'bert.encoder.layer.0.mlp.layernorm.bias', 'bert.encoder.layer.0.mlp.layernorm.weight', 'bert.encoder.layer.0.mlp.wo.bias', 'bert.encoder.layer.0.mlp.wo.weight', 'bert.encoder.layer.1.attention.self.Wqkv.bias', 'bert.encoder.layer.1.attention.self.Wqkv.weight', 'bert.encoder.layer.1.mlp.gated_layers.weight', 'bert.encoder.layer.1.mlp.layernorm.bias', 'bert.encoder.layer.1.mlp.layernorm.weight', 'bert.encoder.layer.1.mlp.wo.bias', 'bert.encoder.layer.1.mlp.wo.weight', 'bert.encoder.layer.10.attention.self.Wqkv.bias', 'bert.encoder.layer.10.attention.self.Wqkv.weight', 'bert.encoder.layer.10.mlp.gated_layers.weight', 'bert.encoder.layer.10.mlp.layernorm.bias', 'bert.encoder.layer.10.mlp.layernorm.weight', 'bert.encoder.layer.10.mlp.wo.bias', 'bert.encoder.layer.10.mlp.wo.weight', 'bert.encoder.layer.11.attention.self.Wqkv.bias', 'bert.encoder.layer.11.attention.self.Wqkv.weight', 'bert.encoder.layer.11.mlp.gated_layers.weight', 'bert.encoder.layer.11.mlp.layernorm.bias', 'bert.encoder.layer.11.mlp.layernorm.weight', 'bert.encoder.layer.11.mlp.wo.bias', 'bert.encoder.layer.11.mlp.wo.weight', 'bert.encoder.layer.2.attention.self.Wqkv.bias', 'bert.encoder.layer.2.attention.self.Wqkv.weight', 'bert.encoder.layer.2.mlp.gated_layers.weight', 'bert.encoder.layer.2.mlp.layernorm.bias', 'bert.encoder.layer.2.mlp.layernorm.weight', 'bert.encoder.layer.2.mlp.wo.bias', 'bert.encoder.layer.2.mlp.wo.weight', 'bert.encoder.layer.3.attention.self.Wqkv.bias', 'bert.encoder.layer.3.attention.self.Wqkv.weight', 'bert.encoder.layer.3.mlp.gated_layers.weight', 'bert.encoder.layer.3.mlp.layernorm.bias', 'bert.encoder.layer.3.mlp.layernorm.weight', 'bert.encoder.layer.3.mlp.wo.bias', 'bert.encoder.layer.3.mlp.wo.weight', 'bert.encoder.layer.4.attention.self.Wqkv.bias', 'bert.encoder.layer.4.attention.self.Wqkv.weight', 'bert.encoder.layer.4.mlp.gated_layers.weight', 'bert.encoder.layer.4.mlp.layernorm.bias', 'bert.encoder.layer.4.mlp.layernorm.weight', 'bert.encoder.layer.4.mlp.wo.bias', 'bert.encoder.layer.4.mlp.wo.weight', 'bert.encoder.layer.5.attention.self.Wqkv.bias', 'bert.encoder.layer.5.attention.self.Wqkv.weight', 'bert.encoder.layer.5.mlp.gated_layers.weight', 'bert.encoder.layer.5.mlp.layernorm.bias', 'bert.encoder.layer.5.mlp.layernorm.weight', 'bert.encoder.layer.5.mlp.wo.bias', 'bert.encoder.layer.5.mlp.wo.weight', 'bert.encoder.layer.6.attention.self.Wqkv.bias', 'bert.encoder.layer.6.attention.self.Wqkv.weight', 'bert.encoder.layer.6.mlp.gated_layers.weight', 'bert.encoder.layer.6.mlp.layernorm.bias', 'bert.encoder.layer.6.mlp.layernorm.weight', 'bert.encoder.layer.6.mlp.wo.bias', 'bert.encoder.layer.6.mlp.wo.weight', 'bert.encoder.layer.7.attention.self.Wqkv.bias', 'bert.encoder.layer.7.attention.self.Wqkv.weight', 'bert.encoder.layer.7.mlp.gated_layers.weight', 'bert.encoder.layer.7.mlp.layernorm.bias', 'bert.encoder.layer.7.mlp.layernorm.weight', 'bert.encoder.layer.7.mlp.wo.bias', 'bert.encoder.layer.7.mlp.wo.weight', 'bert.encoder.layer.8.attention.self.Wqkv.bias', 'bert.encoder.layer.8.attention.self.Wqkv.weight', 'bert.encoder.layer.8.mlp.gated_layers.weight', 'bert.encoder.layer.8.mlp.layernorm.bias', 'bert.encoder.layer.8.mlp.layernorm.weight', 'bert.encoder.layer.8.mlp.wo.bias', 'bert.encoder.layer.8.mlp.wo.weight', 'bert.encoder.layer.9.attention.self.Wqkv.bias', 'bert.encoder.layer.9.attention.self.Wqkv.weight', 'bert.encoder.layer.9.mlp.gated_layers.weight', 'bert.encoder.layer.9.mlp.layernorm.bias', 'bert.encoder.layer.9.mlp.layernorm.weight', 'bert.encoder.layer.9.mlp.wo.bias', 'bert.encoder.layer.9.mlp.wo.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Done grover-pretrained 3
grover-pretrained
Some weights of BertModel were not initialized from the model checkpoint at PoetschLab/GROVER and are newly initialized: ['bert.encoder.layer.0.attention.self.Wqkv.bias', 'bert.encoder.layer.0.attention.self.Wqkv.weight', 'bert.encoder.layer.0.mlp.gated_layers.weight', 'bert.encoder.layer.0.mlp.layernorm.bias', 'bert.encoder.layer.0.mlp.layernorm.weight', 'bert.encoder.layer.0.mlp.wo.bias', 'bert.encoder.layer.0.mlp.wo.weight', 'bert.encoder.layer.1.attention.self.Wqkv.bias', 'bert.encoder.layer.1.attention.self.Wqkv.weight', 'bert.encoder.layer.1.mlp.gated_layers.weight', 'bert.encoder.layer.1.mlp.layernorm.bias', 'bert.encoder.layer.1.mlp.layernorm.weight', 'bert.encoder.layer.1.mlp.wo.bias', 'bert.encoder.layer.1.mlp.wo.weight', 'bert.encoder.layer.10.attention.self.Wqkv.bias', 'bert.encoder.layer.10.attention.self.Wqkv.weight', 'bert.encoder.layer.10.mlp.gated_layers.weight', 'bert.encoder.layer.10.mlp.layernorm.bias', 'bert.encoder.layer.10.mlp.layernorm.weight', 'bert.encoder.layer.10.mlp.wo.bias', 'bert.encoder.layer.10.mlp.wo.weight', 'bert.encoder.layer.11.attention.self.Wqkv.bias', 'bert.encoder.layer.11.attention.self.Wqkv.weight', 'bert.encoder.layer.11.mlp.gated_layers.weight', 'bert.encoder.layer.11.mlp.layernorm.bias', 'bert.encoder.layer.11.mlp.layernorm.weight', 'bert.encoder.layer.11.mlp.wo.bias', 'bert.encoder.layer.11.mlp.wo.weight', 'bert.encoder.layer.2.attention.self.Wqkv.bias', 'bert.encoder.layer.2.attention.self.Wqkv.weight', 'bert.encoder.layer.2.mlp.gated_layers.weight', 'bert.encoder.layer.2.mlp.layernorm.bias', 'bert.encoder.layer.2.mlp.layernorm.weight', 'bert.encoder.layer.2.mlp.wo.bias', 'bert.encoder.layer.2.mlp.wo.weight', 'bert.encoder.layer.3.attention.self.Wqkv.bias', 'bert.encoder.layer.3.attention.self.Wqkv.weight', 'bert.encoder.layer.3.mlp.gated_layers.weight', 'bert.encoder.layer.3.mlp.layernorm.bias', 'bert.encoder.layer.3.mlp.layernorm.weight', 'bert.encoder.layer.3.mlp.wo.bias', 'bert.encoder.layer.3.mlp.wo.weight', 'bert.encoder.layer.4.attention.self.Wqkv.bias', 'bert.encoder.layer.4.attention.self.Wqkv.weight', 'bert.encoder.layer.4.mlp.gated_layers.weight', 'bert.encoder.layer.4.mlp.layernorm.bias', 'bert.encoder.layer.4.mlp.layernorm.weight', 'bert.encoder.layer.4.mlp.wo.bias', 'bert.encoder.layer.4.mlp.wo.weight', 'bert.encoder.layer.5.attention.self.Wqkv.bias', 'bert.encoder.layer.5.attention.self.Wqkv.weight', 'bert.encoder.layer.5.mlp.gated_layers.weight', 'bert.encoder.layer.5.mlp.layernorm.bias', 'bert.encoder.layer.5.mlp.layernorm.weight', 'bert.encoder.layer.5.mlp.wo.bias', 'bert.encoder.layer.5.mlp.wo.weight', 'bert.encoder.layer.6.attention.self.Wqkv.bias', 'bert.encoder.layer.6.attention.self.Wqkv.weight', 'bert.encoder.layer.6.mlp.gated_layers.weight', 'bert.encoder.layer.6.mlp.layernorm.bias', 'bert.encoder.layer.6.mlp.layernorm.weight', 'bert.encoder.layer.6.mlp.wo.bias', 'bert.encoder.layer.6.mlp.wo.weight', 'bert.encoder.layer.7.attention.self.Wqkv.bias', 'bert.encoder.layer.7.attention.self.Wqkv.weight', 'bert.encoder.layer.7.mlp.gated_layers.weight', 'bert.encoder.layer.7.mlp.layernorm.bias', 'bert.encoder.layer.7.mlp.layernorm.weight', 'bert.encoder.layer.7.mlp.wo.bias', 'bert.encoder.layer.7.mlp.wo.weight', 'bert.encoder.layer.8.attention.self.Wqkv.bias', 'bert.encoder.layer.8.attention.self.Wqkv.weight', 'bert.encoder.layer.8.mlp.gated_layers.weight', 'bert.encoder.layer.8.mlp.layernorm.bias', 'bert.encoder.layer.8.mlp.layernorm.weight', 'bert.encoder.layer.8.mlp.wo.bias', 'bert.encoder.layer.8.mlp.wo.weight', 'bert.encoder.layer.9.attention.self.Wqkv.bias', 'bert.encoder.layer.9.attention.self.Wqkv.weight', 'bert.encoder.layer.9.mlp.gated_layers.weight', 'bert.encoder.layer.9.mlp.layernorm.bias', 'bert.encoder.layer.9.mlp.layernorm.weight', 'bert.encoder.layer.9.mlp.wo.bias', 'bert.encoder.layer.9.mlp.wo.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at UKaizokuO/GROVER-finetuned-cancer and are newly initialized: ['bert.encoder.layer.0.attention.self.Wqkv.bias', 'bert.encoder.layer.0.attention.self.Wqkv.weight', 'bert.encoder.layer.0.mlp.gated_layers.weight', 'bert.encoder.layer.0.mlp.layernorm.bias', 'bert.encoder.layer.0.mlp.layernorm.weight', 'bert.encoder.layer.0.mlp.wo.bias', 'bert.encoder.layer.0.mlp.wo.weight', 'bert.encoder.layer.1.attention.self.Wqkv.bias', 'bert.encoder.layer.1.attention.self.Wqkv.weight', 'bert.encoder.layer.1.mlp.gated_layers.weight', 'bert.encoder.layer.1.mlp.layernorm.bias', 'bert.encoder.layer.1.mlp.layernorm.weight', 'bert.encoder.layer.1.mlp.wo.bias', 'bert.encoder.layer.1.mlp.wo.weight', 'bert.encoder.layer.10.attention.self.Wqkv.bias', 'bert.encoder.layer.10.attention.self.Wqkv.weight', 'bert.encoder.layer.10.mlp.gated_layers.weight', 'bert.encoder.layer.10.mlp.layernorm.bias', 'bert.encoder.layer.10.mlp.layernorm.weight', 'bert.encoder.layer.10.mlp.wo.bias', 'bert.encoder.layer.10.mlp.wo.weight', 'bert.encoder.layer.11.attention.self.Wqkv.bias', 'bert.encoder.layer.11.attention.self.Wqkv.weight', 'bert.encoder.layer.11.mlp.gated_layers.weight', 'bert.encoder.layer.11.mlp.layernorm.bias', 'bert.encoder.layer.11.mlp.layernorm.weight', 'bert.encoder.layer.11.mlp.wo.bias', 'bert.encoder.layer.11.mlp.wo.weight', 'bert.encoder.layer.2.attention.self.Wqkv.bias', 'bert.encoder.layer.2.attention.self.Wqkv.weight', 'bert.encoder.layer.2.mlp.gated_layers.weight', 'bert.encoder.layer.2.mlp.layernorm.bias', 'bert.encoder.layer.2.mlp.layernorm.weight', 'bert.encoder.layer.2.mlp.wo.bias', 'bert.encoder.layer.2.mlp.wo.weight', 'bert.encoder.layer.3.attention.self.Wqkv.bias', 'bert.encoder.layer.3.attention.self.Wqkv.weight', 'bert.encoder.layer.3.mlp.gated_layers.weight', 'bert.encoder.layer.3.mlp.layernorm.bias', 'bert.encoder.layer.3.mlp.layernorm.weight', 'bert.encoder.layer.3.mlp.wo.bias', 'bert.encoder.layer.3.mlp.wo.weight', 'bert.encoder.layer.4.attention.self.Wqkv.bias', 'bert.encoder.layer.4.attention.self.Wqkv.weight', 'bert.encoder.layer.4.mlp.gated_layers.weight', 'bert.encoder.layer.4.mlp.layernorm.bias', 'bert.encoder.layer.4.mlp.layernorm.weight', 'bert.encoder.layer.4.mlp.wo.bias', 'bert.encoder.layer.4.mlp.wo.weight', 'bert.encoder.layer.5.attention.self.Wqkv.bias', 'bert.encoder.layer.5.attention.self.Wqkv.weight', 'bert.encoder.layer.5.mlp.gated_layers.weight', 'bert.encoder.layer.5.mlp.layernorm.bias', 'bert.encoder.layer.5.mlp.layernorm.weight', 'bert.encoder.layer.5.mlp.wo.bias', 'bert.encoder.layer.5.mlp.wo.weight', 'bert.encoder.layer.6.attention.self.Wqkv.bias', 'bert.encoder.layer.6.attention.self.Wqkv.weight', 'bert.encoder.layer.6.mlp.gated_layers.weight', 'bert.encoder.layer.6.mlp.layernorm.bias', 'bert.encoder.layer.6.mlp.layernorm.weight', 'bert.encoder.layer.6.mlp.wo.bias', 'bert.encoder.layer.6.mlp.wo.weight', 'bert.encoder.layer.7.attention.self.Wqkv.bias', 'bert.encoder.layer.7.attention.self.Wqkv.weight', 'bert.encoder.layer.7.mlp.gated_layers.weight', 'bert.encoder.layer.7.mlp.layernorm.bias', 'bert.encoder.layer.7.mlp.layernorm.weight', 'bert.encoder.layer.7.mlp.wo.bias', 'bert.encoder.layer.7.mlp.wo.weight', 'bert.encoder.layer.8.attention.self.Wqkv.bias', 'bert.encoder.layer.8.attention.self.Wqkv.weight', 'bert.encoder.layer.8.mlp.gated_layers.weight', 'bert.encoder.layer.8.mlp.layernorm.bias', 'bert.encoder.layer.8.mlp.layernorm.weight', 'bert.encoder.layer.8.mlp.wo.bias', 'bert.encoder.layer.8.mlp.wo.weight', 'bert.encoder.layer.9.attention.self.Wqkv.bias', 'bert.encoder.layer.9.attention.self.Wqkv.weight', 'bert.encoder.layer.9.mlp.gated_layers.weight', 'bert.encoder.layer.9.mlp.layernorm.bias', 'bert.encoder.layer.9.mlp.layernorm.weight', 'bert.encoder.layer.9.mlp.wo.bias', 'bert.encoder.layer.9.mlp.wo.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at UKaizokuO/GROVER-finetuned-cancer and are newly initialized: ['bert.encoder.layer.0.attention.self.Wqkv.bias', 'bert.encoder.layer.0.attention.self.Wqkv.weight', 'bert.encoder.layer.0.mlp.gated_layers.weight', 'bert.encoder.layer.0.mlp.layernorm.bias', 'bert.encoder.layer.0.mlp.layernorm.weight', 'bert.encoder.layer.0.mlp.wo.bias', 'bert.encoder.layer.0.mlp.wo.weight', 'bert.encoder.layer.1.attention.self.Wqkv.bias', 'bert.encoder.layer.1.attention.self.Wqkv.weight', 'bert.encoder.layer.1.mlp.gated_layers.weight', 'bert.encoder.layer.1.mlp.layernorm.bias', 'bert.encoder.layer.1.mlp.layernorm.weight', 'bert.encoder.layer.1.mlp.wo.bias', 'bert.encoder.layer.1.mlp.wo.weight', 'bert.encoder.layer.10.attention.self.Wqkv.bias', 'bert.encoder.layer.10.attention.self.Wqkv.weight', 'bert.encoder.layer.10.mlp.gated_layers.weight', 'bert.encoder.layer.10.mlp.layernorm.bias', 'bert.encoder.layer.10.mlp.layernorm.weight', 'bert.encoder.layer.10.mlp.wo.bias', 'bert.encoder.layer.10.mlp.wo.weight', 'bert.encoder.layer.11.attention.self.Wqkv.bias', 'bert.encoder.layer.11.attention.self.Wqkv.weight', 'bert.encoder.layer.11.mlp.gated_layers.weight', 'bert.encoder.layer.11.mlp.layernorm.bias', 'bert.encoder.layer.11.mlp.layernorm.weight', 'bert.encoder.layer.11.mlp.wo.bias', 'bert.encoder.layer.11.mlp.wo.weight', 'bert.encoder.layer.2.attention.self.Wqkv.bias', 'bert.encoder.layer.2.attention.self.Wqkv.weight', 'bert.encoder.layer.2.mlp.gated_layers.weight', 'bert.encoder.layer.2.mlp.layernorm.bias', 'bert.encoder.layer.2.mlp.layernorm.weight', 'bert.encoder.layer.2.mlp.wo.bias', 'bert.encoder.layer.2.mlp.wo.weight', 'bert.encoder.layer.3.attention.self.Wqkv.bias', 'bert.encoder.layer.3.attention.self.Wqkv.weight', 'bert.encoder.layer.3.mlp.gated_layers.weight', 'bert.encoder.layer.3.mlp.layernorm.bias', 'bert.encoder.layer.3.mlp.layernorm.weight', 'bert.encoder.layer.3.mlp.wo.bias', 'bert.encoder.layer.3.mlp.wo.weight', 'bert.encoder.layer.4.attention.self.Wqkv.bias', 'bert.encoder.layer.4.attention.self.Wqkv.weight', 'bert.encoder.layer.4.mlp.gated_layers.weight', 'bert.encoder.layer.4.mlp.layernorm.bias', 'bert.encoder.layer.4.mlp.layernorm.weight', 'bert.encoder.layer.4.mlp.wo.bias', 'bert.encoder.layer.4.mlp.wo.weight', 'bert.encoder.layer.5.attention.self.Wqkv.bias', 'bert.encoder.layer.5.attention.self.Wqkv.weight', 'bert.encoder.layer.5.mlp.gated_layers.weight', 'bert.encoder.layer.5.mlp.layernorm.bias', 'bert.encoder.layer.5.mlp.layernorm.weight', 'bert.encoder.layer.5.mlp.wo.bias', 'bert.encoder.layer.5.mlp.wo.weight', 'bert.encoder.layer.6.attention.self.Wqkv.bias', 'bert.encoder.layer.6.attention.self.Wqkv.weight', 'bert.encoder.layer.6.mlp.gated_layers.weight', 'bert.encoder.layer.6.mlp.layernorm.bias', 'bert.encoder.layer.6.mlp.layernorm.weight', 'bert.encoder.layer.6.mlp.wo.bias', 'bert.encoder.layer.6.mlp.wo.weight', 'bert.encoder.layer.7.attention.self.Wqkv.bias', 'bert.encoder.layer.7.attention.self.Wqkv.weight', 'bert.encoder.layer.7.mlp.gated_layers.weight', 'bert.encoder.layer.7.mlp.layernorm.bias', 'bert.encoder.layer.7.mlp.layernorm.weight', 'bert.encoder.layer.7.mlp.wo.bias', 'bert.encoder.layer.7.mlp.wo.weight', 'bert.encoder.layer.8.attention.self.Wqkv.bias', 'bert.encoder.layer.8.attention.self.Wqkv.weight', 'bert.encoder.layer.8.mlp.gated_layers.weight', 'bert.encoder.layer.8.mlp.layernorm.bias', 'bert.encoder.layer.8.mlp.layernorm.weight', 'bert.encoder.layer.8.mlp.wo.bias', 'bert.encoder.layer.8.mlp.wo.weight', 'bert.encoder.layer.9.attention.self.Wqkv.bias', 'bert.encoder.layer.9.attention.self.Wqkv.weight', 'bert.encoder.layer.9.mlp.gated_layers.weight', 'bert.encoder.layer.9.mlp.layernorm.bias', 'bert.encoder.layer.9.mlp.layernorm.weight', 'bert.encoder.layer.9.mlp.wo.bias', 'bert.encoder.layer.9.mlp.wo.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at UKaizokuO/GROVER-finetuned-cancer and are newly initialized: ['bert.encoder.layer.0.attention.self.Wqkv.bias', 'bert.encoder.layer.0.attention.self.Wqkv.weight', 'bert.encoder.layer.0.mlp.gated_layers.weight', 'bert.encoder.layer.0.mlp.layernorm.bias', 'bert.encoder.layer.0.mlp.layernorm.weight', 'bert.encoder.layer.0.mlp.wo.bias', 'bert.encoder.layer.0.mlp.wo.weight', 'bert.encoder.layer.1.attention.self.Wqkv.bias', 'bert.encoder.layer.1.attention.self.Wqkv.weight', 'bert.encoder.layer.1.mlp.gated_layers.weight', 'bert.encoder.layer.1.mlp.layernorm.bias', 'bert.encoder.layer.1.mlp.layernorm.weight', 'bert.encoder.layer.1.mlp.wo.bias', 'bert.encoder.layer.1.mlp.wo.weight', 'bert.encoder.layer.10.attention.self.Wqkv.bias', 'bert.encoder.layer.10.attention.self.Wqkv.weight', 'bert.encoder.layer.10.mlp.gated_layers.weight', 'bert.encoder.layer.10.mlp.layernorm.bias', 'bert.encoder.layer.10.mlp.layernorm.weight', 'bert.encoder.layer.10.mlp.wo.bias', 'bert.encoder.layer.10.mlp.wo.weight', 'bert.encoder.layer.11.attention.self.Wqkv.bias', 'bert.encoder.layer.11.attention.self.Wqkv.weight', 'bert.encoder.layer.11.mlp.gated_layers.weight', 'bert.encoder.layer.11.mlp.layernorm.bias', 'bert.encoder.layer.11.mlp.layernorm.weight', 'bert.encoder.layer.11.mlp.wo.bias', 'bert.encoder.layer.11.mlp.wo.weight', 'bert.encoder.layer.2.attention.self.Wqkv.bias', 'bert.encoder.layer.2.attention.self.Wqkv.weight', 'bert.encoder.layer.2.mlp.gated_layers.weight', 'bert.encoder.layer.2.mlp.layernorm.bias', 'bert.encoder.layer.2.mlp.layernorm.weight', 'bert.encoder.layer.2.mlp.wo.bias', 'bert.encoder.layer.2.mlp.wo.weight', 'bert.encoder.layer.3.attention.self.Wqkv.bias', 'bert.encoder.layer.3.attention.self.Wqkv.weight', 'bert.encoder.layer.3.mlp.gated_layers.weight', 'bert.encoder.layer.3.mlp.layernorm.bias', 'bert.encoder.layer.3.mlp.layernorm.weight', 'bert.encoder.layer.3.mlp.wo.bias', 'bert.encoder.layer.3.mlp.wo.weight', 'bert.encoder.layer.4.attention.self.Wqkv.bias', 'bert.encoder.layer.4.attention.self.Wqkv.weight', 'bert.encoder.layer.4.mlp.gated_layers.weight', 'bert.encoder.layer.4.mlp.layernorm.bias', 'bert.encoder.layer.4.mlp.layernorm.weight', 'bert.encoder.layer.4.mlp.wo.bias', 'bert.encoder.layer.4.mlp.wo.weight', 'bert.encoder.layer.5.attention.self.Wqkv.bias', 'bert.encoder.layer.5.attention.self.Wqkv.weight', 'bert.encoder.layer.5.mlp.gated_layers.weight', 'bert.encoder.layer.5.mlp.layernorm.bias', 'bert.encoder.layer.5.mlp.layernorm.weight', 'bert.encoder.layer.5.mlp.wo.bias', 'bert.encoder.layer.5.mlp.wo.weight', 'bert.encoder.layer.6.attention.self.Wqkv.bias', 'bert.encoder.layer.6.attention.self.Wqkv.weight', 'bert.encoder.layer.6.mlp.gated_layers.weight', 'bert.encoder.layer.6.mlp.layernorm.bias', 'bert.encoder.layer.6.mlp.layernorm.weight', 'bert.encoder.layer.6.mlp.wo.bias', 'bert.encoder.layer.6.mlp.wo.weight', 'bert.encoder.layer.7.attention.self.Wqkv.bias', 'bert.encoder.layer.7.attention.self.Wqkv.weight', 'bert.encoder.layer.7.mlp.gated_layers.weight', 'bert.encoder.layer.7.mlp.layernorm.bias', 'bert.encoder.layer.7.mlp.layernorm.weight', 'bert.encoder.layer.7.mlp.wo.bias', 'bert.encoder.layer.7.mlp.wo.weight', 'bert.encoder.layer.8.attention.self.Wqkv.bias', 'bert.encoder.layer.8.attention.self.Wqkv.weight', 'bert.encoder.layer.8.mlp.gated_layers.weight', 'bert.encoder.layer.8.mlp.layernorm.bias', 'bert.encoder.layer.8.mlp.layernorm.weight', 'bert.encoder.layer.8.mlp.wo.bias', 'bert.encoder.layer.8.mlp.wo.weight', 'bert.encoder.layer.9.attention.self.Wqkv.bias', 'bert.encoder.layer.9.attention.self.Wqkv.weight', 'bert.encoder.layer.9.mlp.gated_layers.weight', 'bert.encoder.layer.9.mlp.layernorm.bias', 'bert.encoder.layer.9.mlp.layernorm.weight', 'bert.encoder.layer.9.mlp.wo.bias', 'bert.encoder.layer.9.mlp.wo.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at UKaizokuO/GROVER-finetuned-cancer and are newly initialized: ['bert.encoder.layer.0.attention.self.Wqkv.bias', 'bert.encoder.layer.0.attention.self.Wqkv.weight', 'bert.encoder.layer.0.mlp.gated_layers.weight', 'bert.encoder.layer.0.mlp.layernorm.bias', 'bert.encoder.layer.0.mlp.layernorm.weight', 'bert.encoder.layer.0.mlp.wo.bias', 'bert.encoder.layer.0.mlp.wo.weight', 'bert.encoder.layer.1.attention.self.Wqkv.bias', 'bert.encoder.layer.1.attention.self.Wqkv.weight', 'bert.encoder.layer.1.mlp.gated_layers.weight', 'bert.encoder.layer.1.mlp.layernorm.bias', 'bert.encoder.layer.1.mlp.layernorm.weight', 'bert.encoder.layer.1.mlp.wo.bias', 'bert.encoder.layer.1.mlp.wo.weight', 'bert.encoder.layer.10.attention.self.Wqkv.bias', 'bert.encoder.layer.10.attention.self.Wqkv.weight', 'bert.encoder.layer.10.mlp.gated_layers.weight', 'bert.encoder.layer.10.mlp.layernorm.bias', 'bert.encoder.layer.10.mlp.layernorm.weight', 'bert.encoder.layer.10.mlp.wo.bias', 'bert.encoder.layer.10.mlp.wo.weight', 'bert.encoder.layer.11.attention.self.Wqkv.bias', 'bert.encoder.layer.11.attention.self.Wqkv.weight', 'bert.encoder.layer.11.mlp.gated_layers.weight', 'bert.encoder.layer.11.mlp.layernorm.bias', 'bert.encoder.layer.11.mlp.layernorm.weight', 'bert.encoder.layer.11.mlp.wo.bias', 'bert.encoder.layer.11.mlp.wo.weight', 'bert.encoder.layer.2.attention.self.Wqkv.bias', 'bert.encoder.layer.2.attention.self.Wqkv.weight', 'bert.encoder.layer.2.mlp.gated_layers.weight', 'bert.encoder.layer.2.mlp.layernorm.bias', 'bert.encoder.layer.2.mlp.layernorm.weight', 'bert.encoder.layer.2.mlp.wo.bias', 'bert.encoder.layer.2.mlp.wo.weight', 'bert.encoder.layer.3.attention.self.Wqkv.bias', 'bert.encoder.layer.3.attention.self.Wqkv.weight', 'bert.encoder.layer.3.mlp.gated_layers.weight', 'bert.encoder.layer.3.mlp.layernorm.bias', 'bert.encoder.layer.3.mlp.layernorm.weight', 'bert.encoder.layer.3.mlp.wo.bias', 'bert.encoder.layer.3.mlp.wo.weight', 'bert.encoder.layer.4.attention.self.Wqkv.bias', 'bert.encoder.layer.4.attention.self.Wqkv.weight', 'bert.encoder.layer.4.mlp.gated_layers.weight', 'bert.encoder.layer.4.mlp.layernorm.bias', 'bert.encoder.layer.4.mlp.layernorm.weight', 'bert.encoder.layer.4.mlp.wo.bias', 'bert.encoder.layer.4.mlp.wo.weight', 'bert.encoder.layer.5.attention.self.Wqkv.bias', 'bert.encoder.layer.5.attention.self.Wqkv.weight', 'bert.encoder.layer.5.mlp.gated_layers.weight', 'bert.encoder.layer.5.mlp.layernorm.bias', 'bert.encoder.layer.5.mlp.layernorm.weight', 'bert.encoder.layer.5.mlp.wo.bias', 'bert.encoder.layer.5.mlp.wo.weight', 'bert.encoder.layer.6.attention.self.Wqkv.bias', 'bert.encoder.layer.6.attention.self.Wqkv.weight', 'bert.encoder.layer.6.mlp.gated_layers.weight', 'bert.encoder.layer.6.mlp.layernorm.bias', 'bert.encoder.layer.6.mlp.layernorm.weight', 'bert.encoder.layer.6.mlp.wo.bias', 'bert.encoder.layer.6.mlp.wo.weight', 'bert.encoder.layer.7.attention.self.Wqkv.bias', 'bert.encoder.layer.7.attention.self.Wqkv.weight', 'bert.encoder.layer.7.mlp.gated_layers.weight', 'bert.encoder.layer.7.mlp.layernorm.bias', 'bert.encoder.layer.7.mlp.layernorm.weight', 'bert.encoder.layer.7.mlp.wo.bias', 'bert.encoder.layer.7.mlp.wo.weight', 'bert.encoder.layer.8.attention.self.Wqkv.bias', 'bert.encoder.layer.8.attention.self.Wqkv.weight', 'bert.encoder.layer.8.mlp.gated_layers.weight', 'bert.encoder.layer.8.mlp.layernorm.bias', 'bert.encoder.layer.8.mlp.layernorm.weight', 'bert.encoder.layer.8.mlp.wo.bias', 'bert.encoder.layer.8.mlp.wo.weight', 'bert.encoder.layer.9.attention.self.Wqkv.bias', 'bert.encoder.layer.9.attention.self.Wqkv.weight', 'bert.encoder.layer.9.mlp.gated_layers.weight', 'bert.encoder.layer.9.mlp.layernorm.bias', 'bert.encoder.layer.9.mlp.layernorm.weight', 'bert.encoder.layer.9.mlp.wo.bias', 'bert.encoder.layer.9.mlp.wo.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Done grover-pretrained 4
grover-finetuned-cancer
Done grover-finetuned-cancer 1
grover-finetuned-cancer
Done grover-finetuned-cancer 2
grover-finetuned-cancer
Done grover-finetuned-cancer 3
grover-finetuned-cancer
Done grover-finetuned-cancer 4
