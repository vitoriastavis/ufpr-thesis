> datasets loaded
Embedding w2v-bpe took 429.211s
Embedding args:
vocab_size: 100; window_size: 5; 

> model loaded
Learning rate: 0.003
Number of epochs: 100
Hidden size: 768

> training complete
Epoch 0/100, loss = 0.0
Epoch 100/100, loss = 0.68

> evaluation complete
Accuracy: 0.5718918918918919
Precision: 0.5869164767296522
Recall: 0.5718846405847143
F1-score: 0.55
Matthew's correlation: 0.15808807280248233
