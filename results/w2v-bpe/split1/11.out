> datasets loaded
Embedding w2v-bpe took 1017.946s
Embedding args:
vocab_size: 200; window_size: 5; 

> model loaded
Learning rate: 0.0003
Number of epochs: 20
Hidden size: 768

> training complete
Epoch 0/20, loss = 0.0
Epoch 20/20, loss = 0.688

> evaluation complete
Accuracy: 0.5647079337401918
Precision: 0.5734534289367522
Recall: 0.5647139483718508
F1-score: 0.55
Matthew's correlation: 0.13789070175974014
