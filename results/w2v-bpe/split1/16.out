> datasets loaded
Embedding w2v-bpe took 1074.041s
Embedding args:
vocab_size: 200; window_size: 10; 

> model loaded
Learning rate: 0.0003
Number of epochs: 100
Hidden size: 768

> training complete
Epoch 0/100, loss = 0.0
Epoch 100/100, loss = 0.684

> evaluation complete
Accuracy: 0.570566695727986
Precision: 0.5720120648691487
Recall: 0.5705691640243497
F1-score: 0.57
Matthew's correlation: 0.14257392773551644
