nohup: ignoring input
/home/stavisa/.local/lib/python3.10/site-packages/torch/autograd/graph.py:825: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Done dnabert1 25
Done dnabert1 26
Done dnabert1 27
Done dnabert1 28
Done dnabert1 29
Done dnabert1 30
Done dnabert1 31
Done dnabert1 32
Done dnabert1 33
Done dnabert1 34
Done dnabert1 35
Done dnabert1 36
Done dnabert1 37
Done dnabert1 38
Done dnabert1 39
Done dnabert1 40
Done dnabert1 41
Done dnabert1 42
Done dnabert1 43
Done dnabert1 44
Done dnabert1 45
Done dnabert1 46
Done dnabert1 47
/home/stavisa/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).
  warnings.warn(
Done dnabert1 48
/home/stavisa/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).
  warnings.warn(
Done dnabert2 25
/home/stavisa/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).
  warnings.warn(
Done dnabert2 26
/home/stavisa/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).
  warnings.warn(
Done dnabert2 27
/home/stavisa/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).
  warnings.warn(
Done dnabert2 28
/home/stavisa/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).
  warnings.warn(
Done dnabert2 29
/home/stavisa/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).
  warnings.warn(
Done dnabert2 30
/home/stavisa/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).
  warnings.warn(
Done dnabert2 31
/home/stavisa/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).
  warnings.warn(
Done dnabert2 32
/home/stavisa/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).
  warnings.warn(
Done dnabert2 33
/home/stavisa/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).
  warnings.warn(
Done dnabert2 34
/home/stavisa/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).
  warnings.warn(
Done dnabert2 35
/home/stavisa/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).
  warnings.warn(
Done dnabert2 36
/home/stavisa/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).
  warnings.warn(
Done dnabert2 37
/home/stavisa/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).
  warnings.warn(
Done dnabert2 38
/home/stavisa/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).
  warnings.warn(
Done dnabert2 39
/home/stavisa/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).
  warnings.warn(
Done dnabert2 40
/home/stavisa/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).
  warnings.warn(
Done dnabert2 41
/home/stavisa/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).
  warnings.warn(
Done dnabert2 42
/home/stavisa/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).
  warnings.warn(
Done dnabert2 43
/home/stavisa/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).
  warnings.warn(
Done dnabert2 44
/home/stavisa/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).
  warnings.warn(
Done dnabert2 45
/home/stavisa/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).
  warnings.warn(
Done dnabert2 46
/home/stavisa/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).
  warnings.warn(
Done dnabert2 47
/home/stavisa/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).
  warnings.warn(
Some weights of BertModel were not initialized from the model checkpoint at UKaizokuO/GROVER-finetuned-cancer and are newly initialized: ['bert.encoder.layer.0.attention.self.Wqkv.bias', 'bert.encoder.layer.0.attention.self.Wqkv.weight', 'bert.encoder.layer.0.mlp.gated_layers.weight', 'bert.encoder.layer.0.mlp.layernorm.bias', 'bert.encoder.layer.0.mlp.layernorm.weight', 'bert.encoder.layer.0.mlp.wo.bias', 'bert.encoder.layer.0.mlp.wo.weight', 'bert.encoder.layer.1.attention.self.Wqkv.bias', 'bert.encoder.layer.1.attention.self.Wqkv.weight', 'bert.encoder.layer.1.mlp.gated_layers.weight', 'bert.encoder.layer.1.mlp.layernorm.bias', 'bert.encoder.layer.1.mlp.layernorm.weight', 'bert.encoder.layer.1.mlp.wo.bias', 'bert.encoder.layer.1.mlp.wo.weight', 'bert.encoder.layer.10.attention.self.Wqkv.bias', 'bert.encoder.layer.10.attention.self.Wqkv.weight', 'bert.encoder.layer.10.mlp.gated_layers.weight', 'bert.encoder.layer.10.mlp.layernorm.bias', 'bert.encoder.layer.10.mlp.layernorm.weight', 'bert.encoder.layer.10.mlp.wo.bias', 'bert.encoder.layer.10.mlp.wo.weight', 'bert.encoder.layer.11.attention.self.Wqkv.bias', 'bert.encoder.layer.11.attention.self.Wqkv.weight', 'bert.encoder.layer.11.mlp.gated_layers.weight', 'bert.encoder.layer.11.mlp.layernorm.bias', 'bert.encoder.layer.11.mlp.layernorm.weight', 'bert.encoder.layer.11.mlp.wo.bias', 'bert.encoder.layer.11.mlp.wo.weight', 'bert.encoder.layer.2.attention.self.Wqkv.bias', 'bert.encoder.layer.2.attention.self.Wqkv.weight', 'bert.encoder.layer.2.mlp.gated_layers.weight', 'bert.encoder.layer.2.mlp.layernorm.bias', 'bert.encoder.layer.2.mlp.layernorm.weight', 'bert.encoder.layer.2.mlp.wo.bias', 'bert.encoder.layer.2.mlp.wo.weight', 'bert.encoder.layer.3.attention.self.Wqkv.bias', 'bert.encoder.layer.3.attention.self.Wqkv.weight', 'bert.encoder.layer.3.mlp.gated_layers.weight', 'bert.encoder.layer.3.mlp.layernorm.bias', 'bert.encoder.layer.3.mlp.layernorm.weight', 'bert.encoder.layer.3.mlp.wo.bias', 'bert.encoder.layer.3.mlp.wo.weight', 'bert.encoder.layer.4.attention.self.Wqkv.bias', 'bert.encoder.layer.4.attention.self.Wqkv.weight', 'bert.encoder.layer.4.mlp.gated_layers.weight', 'bert.encoder.layer.4.mlp.layernorm.bias', 'bert.encoder.layer.4.mlp.layernorm.weight', 'bert.encoder.layer.4.mlp.wo.bias', 'bert.encoder.layer.4.mlp.wo.weight', 'bert.encoder.layer.5.attention.self.Wqkv.bias', 'bert.encoder.layer.5.attention.self.Wqkv.weight', 'bert.encoder.layer.5.mlp.gated_layers.weight', 'bert.encoder.layer.5.mlp.layernorm.bias', 'bert.encoder.layer.5.mlp.layernorm.weight', 'bert.encoder.layer.5.mlp.wo.bias', 'bert.encoder.layer.5.mlp.wo.weight', 'bert.encoder.layer.6.attention.self.Wqkv.bias', 'bert.encoder.layer.6.attention.self.Wqkv.weight', 'bert.encoder.layer.6.mlp.gated_layers.weight', 'bert.encoder.layer.6.mlp.layernorm.bias', 'bert.encoder.layer.6.mlp.layernorm.weight', 'bert.encoder.layer.6.mlp.wo.bias', 'bert.encoder.layer.6.mlp.wo.weight', 'bert.encoder.layer.7.attention.self.Wqkv.bias', 'bert.encoder.layer.7.attention.self.Wqkv.weight', 'bert.encoder.layer.7.mlp.gated_layers.weight', 'bert.encoder.layer.7.mlp.layernorm.bias', 'bert.encoder.layer.7.mlp.layernorm.weight', 'bert.encoder.layer.7.mlp.wo.bias', 'bert.encoder.layer.7.mlp.wo.weight', 'bert.encoder.layer.8.attention.self.Wqkv.bias', 'bert.encoder.layer.8.attention.self.Wqkv.weight', 'bert.encoder.layer.8.mlp.gated_layers.weight', 'bert.encoder.layer.8.mlp.layernorm.bias', 'bert.encoder.layer.8.mlp.layernorm.weight', 'bert.encoder.layer.8.mlp.wo.bias', 'bert.encoder.layer.8.mlp.wo.weight', 'bert.encoder.layer.9.attention.self.Wqkv.bias', 'bert.encoder.layer.9.attention.self.Wqkv.weight', 'bert.encoder.layer.9.mlp.gated_layers.weight', 'bert.encoder.layer.9.mlp.layernorm.bias', 'bert.encoder.layer.9.mlp.layernorm.weight', 'bert.encoder.layer.9.mlp.wo.bias', 'bert.encoder.layer.9.mlp.wo.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Done dnabert2 48
/home/stavisa/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).
  warnings.warn(
Some weights of BertModel were not initialized from the model checkpoint at UKaizokuO/GROVER-finetuned-cancer and are newly initialized: ['bert.encoder.layer.0.attention.self.Wqkv.bias', 'bert.encoder.layer.0.attention.self.Wqkv.weight', 'bert.encoder.layer.0.mlp.gated_layers.weight', 'bert.encoder.layer.0.mlp.layernorm.bias', 'bert.encoder.layer.0.mlp.layernorm.weight', 'bert.encoder.layer.0.mlp.wo.bias', 'bert.encoder.layer.0.mlp.wo.weight', 'bert.encoder.layer.1.attention.self.Wqkv.bias', 'bert.encoder.layer.1.attention.self.Wqkv.weight', 'bert.encoder.layer.1.mlp.gated_layers.weight', 'bert.encoder.layer.1.mlp.layernorm.bias', 'bert.encoder.layer.1.mlp.layernorm.weight', 'bert.encoder.layer.1.mlp.wo.bias', 'bert.encoder.layer.1.mlp.wo.weight', 'bert.encoder.layer.10.attention.self.Wqkv.bias', 'bert.encoder.layer.10.attention.self.Wqkv.weight', 'bert.encoder.layer.10.mlp.gated_layers.weight', 'bert.encoder.layer.10.mlp.layernorm.bias', 'bert.encoder.layer.10.mlp.layernorm.weight', 'bert.encoder.layer.10.mlp.wo.bias', 'bert.encoder.layer.10.mlp.wo.weight', 'bert.encoder.layer.11.attention.self.Wqkv.bias', 'bert.encoder.layer.11.attention.self.Wqkv.weight', 'bert.encoder.layer.11.mlp.gated_layers.weight', 'bert.encoder.layer.11.mlp.layernorm.bias', 'bert.encoder.layer.11.mlp.layernorm.weight', 'bert.encoder.layer.11.mlp.wo.bias', 'bert.encoder.layer.11.mlp.wo.weight', 'bert.encoder.layer.2.attention.self.Wqkv.bias', 'bert.encoder.layer.2.attention.self.Wqkv.weight', 'bert.encoder.layer.2.mlp.gated_layers.weight', 'bert.encoder.layer.2.mlp.layernorm.bias', 'bert.encoder.layer.2.mlp.layernorm.weight', 'bert.encoder.layer.2.mlp.wo.bias', 'bert.encoder.layer.2.mlp.wo.weight', 'bert.encoder.layer.3.attention.self.Wqkv.bias', 'bert.encoder.layer.3.attention.self.Wqkv.weight', 'bert.encoder.layer.3.mlp.gated_layers.weight', 'bert.encoder.layer.3.mlp.layernorm.bias', 'bert.encoder.layer.3.mlp.layernorm.weight', 'bert.encoder.layer.3.mlp.wo.bias', 'bert.encoder.layer.3.mlp.wo.weight', 'bert.encoder.layer.4.attention.self.Wqkv.bias', 'bert.encoder.layer.4.attention.self.Wqkv.weight', 'bert.encoder.layer.4.mlp.gated_layers.weight', 'bert.encoder.layer.4.mlp.layernorm.bias', 'bert.encoder.layer.4.mlp.layernorm.weight', 'bert.encoder.layer.4.mlp.wo.bias', 'bert.encoder.layer.4.mlp.wo.weight', 'bert.encoder.layer.5.attention.self.Wqkv.bias', 'bert.encoder.layer.5.attention.self.Wqkv.weight', 'bert.encoder.layer.5.mlp.gated_layers.weight', 'bert.encoder.layer.5.mlp.layernorm.bias', 'bert.encoder.layer.5.mlp.layernorm.weight', 'bert.encoder.layer.5.mlp.wo.bias', 'bert.encoder.layer.5.mlp.wo.weight', 'bert.encoder.layer.6.attention.self.Wqkv.bias', 'bert.encoder.layer.6.attention.self.Wqkv.weight', 'bert.encoder.layer.6.mlp.gated_layers.weight', 'bert.encoder.layer.6.mlp.layernorm.bias', 'bert.encoder.layer.6.mlp.layernorm.weight', 'bert.encoder.layer.6.mlp.wo.bias', 'bert.encoder.layer.6.mlp.wo.weight', 'bert.encoder.layer.7.attention.self.Wqkv.bias', 'bert.encoder.layer.7.attention.self.Wqkv.weight', 'bert.encoder.layer.7.mlp.gated_layers.weight', 'bert.encoder.layer.7.mlp.layernorm.bias', 'bert.encoder.layer.7.mlp.layernorm.weight', 'bert.encoder.layer.7.mlp.wo.bias', 'bert.encoder.layer.7.mlp.wo.weight', 'bert.encoder.layer.8.attention.self.Wqkv.bias', 'bert.encoder.layer.8.attention.self.Wqkv.weight', 'bert.encoder.layer.8.mlp.gated_layers.weight', 'bert.encoder.layer.8.mlp.layernorm.bias', 'bert.encoder.layer.8.mlp.layernorm.weight', 'bert.encoder.layer.8.mlp.wo.bias', 'bert.encoder.layer.8.mlp.wo.weight', 'bert.encoder.layer.9.attention.self.Wqkv.bias', 'bert.encoder.layer.9.attention.self.Wqkv.weight', 'bert.encoder.layer.9.mlp.gated_layers.weight', 'bert.encoder.layer.9.mlp.layernorm.bias', 'bert.encoder.layer.9.mlp.layernorm.weight', 'bert.encoder.layer.9.mlp.wo.bias', 'bert.encoder.layer.9.mlp.wo.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
